{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrated 3 ways to perform medical named entity recognition from a pdf and save retrieved entities in a file\n",
        "NER Way 1 :-using spacy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfBg9cDV-28M",
        "outputId": "000dfbd8-bba8-4bfa-ddf6-99902c64ef0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted text has been saved to '/content/sample_data/extracted_text.txt'\n",
            "Total words in the extracted text: 314\n"
          ]
        }
      ],
      "source": [
        "import fitz\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "## Clean the text\n",
        "def clean_and_tokenize(text):\n",
        "    doc = nlp(text)\n",
        "    clean_text = \" \".join(token.text for token in doc if not token.is_space)\n",
        "    return clean_text\n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            text += page.get_text()\n",
        "        doc.close()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def save_text_to_file(text, output_file):\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "        print(f\"Extracted text has been saved to '{output_file}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving text to file: {str(e)}\")\n",
        "\n",
        "def count_words_in_text(text):\n",
        "    if text:\n",
        "        word_count = len(text.split())\n",
        "        print(f\"Total words in the extracted text: {word_count}\")\n",
        "    else:\n",
        "        print(\"Cannot count words. Text is empty or extraction failed.\")\n",
        "\n",
        "# Example usage:\n",
        "pdf_path = '/content/sample_data/patients.pdf'  # Replace with your PDF file path\n",
        "output_file = '/content/sample_data/extracted_text.txt'\n",
        "\n",
        "# Extract text from PDF\n",
        "extracted_text = pdf_to_text(pdf_path)\n",
        "cleaned_text = clean_and_tokenize(extracted_text)\n",
        "\n",
        "# Save extracted text to a file\n",
        "if cleaned_text:\n",
        "    save_text_to_file(cleaned_text, output_file)\n",
        "\n",
        "# Perform operations on the extracted text\n",
        "count_words_in_text(cleaned_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prsTWLTwUcKE"
      },
      "outputs": [],
      "source": [
        "Way 2 : Using open source model from Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHImKkRvWEzW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def read_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "        return text\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error reading file: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {str(e)}\")\n",
        "\n",
        "file_path = '/content/sample_data/extracted_text.txt'  # Replace with your file path\n",
        "file_text = read_file(file_path)\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\", model_max_length=512)\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
        "\n",
        "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "result = pipe(file_text)\n",
        "\n",
        "\n",
        "grouped_entities = {}\n",
        "\n",
        "# Group named entities by labels\n",
        "for entity in result:\n",
        "    label = entity[\"entity_group\"]\n",
        "    word = entity[\"word\"]\n",
        "    if label not in grouped_entities:\n",
        "        grouped_entities[label] = []\n",
        "    grouped_entities[label].append(word)\n",
        "\n",
        "# Print named entities grouped by labels\n",
        "for label, entities in grouped_entities.items():\n",
        "    print(label + \"_ =\", \", \".join(entities))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Severity_ = severe\n",
        "Sign_symptom_ = di, ##zziness, symptoms, rash, it, ##ching, swelling, symptoms, fever, symptoms, dark, adverse effects, short, symptoms, ins, tremor, anxiety, symptoms, headache, symptoms\n",
        "Medication_ = ibuprofen, am, at, ##orvastat, met, ##op, met, ##oprol, al, ##pr, ##azolam, top\n",
        "Time_ = within two hours\n",
        "Duration_ = three days, the course of a week, three days, a few days, a two - week period, four - week, one week, two weeks\n",
        "Biological_structure_ = skin, muscle, chest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Way 3 :-Using GLiNER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g-tmVz8WO-n",
        "outputId": "8725299e-b7b8-4ecd-c79e-90ce05e88669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diseases:\n",
            "- cardiac conditions\n",
            "\n",
            "Symptomss:\n",
            "- nausea\n",
            "- vomiting\n",
            "- headache\n",
            "- dizziness\n",
            "- skin rash\n",
            "- itching\n",
            "- abdominal pain\n",
            "- diarrhea\n",
            "- fever\n",
            "- muscle cramps\n",
            "- weakness\n",
            "- dark urine\n",
            "- palpitations\n",
            "- shortness of breath\n",
            "- chest pain\n",
            "- insomnia\n",
            "- agitation\n",
            "- tremors\n",
            "- blurred vision\n",
            "- confusion\n",
            "\n",
            "Drugs:\n",
            "- Ibuprofen\n",
            "- Penicillin\n",
            "- Amoxicillin\n",
            "- Atorvastatin\n",
            "- Metoprolol\n",
            "- Metoprolol\n",
            "- Alprazolam\n",
            "- Topiramate\n",
            "\n",
            "Medical treatments:\n",
            "- cholesterol management\n",
            "- anxiety management\n",
            "- migraine prophylaxis\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from gliner import GLiNER\n",
        "\n",
        "# Initialize GLiNER with the base model\n",
        "model = GLiNER.from_pretrained(\"urchade/gliner_large_bio-v0.1\")\n",
        "\n",
        "#Sample text for entity prediction\n",
        "text = file_text\n",
        "\n",
        "# Labels for entity prediction\n",
        "labels = [\"Disease\",\"Symptoms\",\"Drug\",\"Medical treatment\"] # for v2.1 use capital case for better performance\n",
        "\n",
        "# Perform entity prediction\n",
        "entities = model.predict_entities(text, labels, threshold=0.5)\n",
        "\n",
        "# Initialize dictionaries to store entities by label\n",
        "predicted_entities = {label: [] for label in labels}\n",
        "\n",
        "# Group entities by their labels\n",
        "for entity in entities:\n",
        "    label = entity[\"label\"]\n",
        "    predicted_entities[label].append(entity[\"text\"])\n",
        "\n",
        "# Display predicted entities grouped by labels\n",
        "for label, entities_list in predicted_entities.items():\n",
        "    if entities_list:\n",
        "        print(f\"{label}s:\")\n",
        "        for entity in entities_list:\n",
        "            print(f\"- {entity}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Diseases:\n",
        "- cardiac conditions\n",
        "\n",
        "Symptomss:\n",
        "- nausea\n",
        "- vomiting\n",
        "- headache\n",
        "- dizziness\n",
        "- skin rash\n",
        "- itching\n",
        "- abdominal pain\n",
        "- diarrhea\n",
        "- fever\n",
        "- muscle cramps\n",
        "- weakness\n",
        "- dark urine\n",
        "- palpitations\n",
        "- shortness of breath\n",
        "- chest pain\n",
        "- insomnia\n",
        "- agitation\n",
        "- tremors\n",
        "- blurred vision\n",
        "- confusion"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
